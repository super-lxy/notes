



## windows



下载miniconda3

```cmd
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe -o .\miniconda.exe
start /wait "" .\miniconda.exe /S
del .\miniconda.exe
```



下载LLaMa-Factory项目

```cmd
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
conda create -n llama_factory python=3.10
conda activate llama_factory
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
#安装cuda版本pytorch
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126

#bitsandbytes windows编译
#地址
https://github.com/bitsandbytes-foundation/bitsandbytes
pip install -U bitsandbytes


```







## Linux

必备：CUDA、显卡驱动 、Miniconda or Conda



部署项目

```sh
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
conda create -n llama_factory python=3.10
conda activate llama_factory
cd LLaMA-Factory
pip install -e ".[torch,metrics]"

torch.cuda.current_device()
torch.cuda.get_device_name(0)
torch.__version__


CUDA_VISIBLE_DEVICES=0 llamafactory-cli 
```



**[flash-attention](https://github.com/Dao-AILab/flash-attention)**

```cmd
flash_attn-2.7.4.post1+cu12torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

#cu12 指定cuda版本为12.xx
#torch2.2  指定 torch 2.2
#cp310  指定python 版本为 3.10

```



**[unsloth](https://github.com/unslothai/unsloth)**

```cmd
pip install unsloth
```


下载模型

国外：huggingface
国内：魔塔社区





12.8 pre torch





## ollama

```sh
(llama_factory) ➜  LLaMA-Factory-main ollama list             
NAME            ID              SIZE      MODIFIED       
qwen2.5vl:3b    fb90415cde1e    3.2 GB    44 seconds ago    
qwen3:0.6b      7df6b6e09427    522 MB    6 hours ago 

(llama_factory) ➜  LLaMA-Factory-main ollama show --modelfile qwen2.5vl:3b 
```

qwen25vl

![image-20250702203506466](LLaMa-Factory.assets/image-20250702203506466.png)
qwen3

![image-20250702203654988](LLaMa-Factory.assets/image-20250702203654988.png)

```modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM qwen2.5vl:3b

FROM ./xxx.gguf
TEMPLATE """
xx从ollama上找模型的templatexxx
"""
#else parameters

#qwen25vl
SYSTEM You are a helpful assistant.
#params
PARAMETER temperature 0.0001


##qwen3
PARAMETER top_k 20
PARAMETER top_p 0.95
PARAMETER repeat_penalty 1
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>
PARAMETER temperature 0.6

```

